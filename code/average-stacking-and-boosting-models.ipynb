{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.linear_model import ElasticNetCV, LassoLarsCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class StackingEstimator(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        self.estimator.fit(X, y, **fit_params)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = check_array(X)\n",
    "        X_transformed = np.copy(X)\n",
    "        # add class probabilities as a synthetic feature\n",
    "        if issubclass(self.estimator.__class__, ClassifierMixin) and hasattr(self.estimator, 'predict_proba'):\n",
    "            X_transformed = np.hstack((self.estimator.predict_proba(X), X))\n",
    "\n",
    "        # add class prodiction as a synthetic feature\n",
    "        X_transformed = np.hstack((np.reshape(self.estimator.predict(X), (-1, 1)), X_transformed))\n",
    "\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoder\n",
    "\n",
    "Simply speaking, `LabelEncoder` is used to simplify the continuous feature such as temperature.\n",
    "\n",
    "### Examples\n",
    "\n",
    "LabelEncoder can be used to normalize labels.\n",
    "\n",
    "```python\n",
    ">>> from sklearn import preprocessing\n",
    ">>> le = preprocessing.LabelEncoder()\n",
    ">>> le.fit([1, 2, 2, 6])\n",
    "LabelEncoder()\n",
    ">>> le.classes_\n",
    "array([1, 2, 6])\n",
    ">>> le.transform([1, 1, 2, 6]) \n",
    "array([0, 0, 1, 2]...)\n",
    ">>> le.inverse_transform([0, 0, 1, 2])\n",
    "array([1, 1, 2, 6])\n",
    "```\n",
    "\n",
    "It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels.\n",
    "\n",
    "```Python\n",
    ">>> le = preprocessing.LabelEncoder()\n",
    ">>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
    "LabelEncoder()\n",
    ">>> list(le.classes_)\n",
    "['amsterdam', 'paris', 'tokyo']\n",
    ">>> le.transform([\"tokyo\", \"tokyo\", \"paris\"]) \n",
    "array([2, 2, 1]...)\n",
    ">>> list(le.inverse_transform([2, 2, 1]))\n",
    "['tokyo', 'tokyo', 'paris']\n",
    "```\n",
    "\n",
    "Besides, there're also `OneHotEncoder` function: `sklearn.preprocessing.OneHotEncoder`, encode categorical integer features using a one-hot aka one-of-K scheme.\n",
    "```Python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit([[1],[2],[3],[4]])\n",
    "ohe.transform([2],[3],[1],[4]).toarray()\n",
    ">>>[ [0,1,0,0] , [0,0,1,0] , [1,0,0,0] ,[0,0,0,1] ]\n",
    "```\n",
    "\n",
    "ref: sklearn.preprocessing.LabelEncoder â€” scikit-learn 0.18.2 documentation  \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# Load data and encode non-value features  #\n",
    "############################################\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "############################\n",
    "# Encode non-value label   #\n",
    "############################\n",
    "for c in train.columns:\n",
    "    if train[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(train[c].values) + list(test[c].values))\n",
    "        train[c] = lbl.transform(list(train[c].values))\n",
    "        test[c] = lbl.transform(list(test[c].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Using Dimension Reduction\n",
    "\n",
    "Mainly using these methods below:\n",
    "\n",
    "* TruncatedSVD (tSVD)\n",
    "* Principal Component Analysis (PCA)\n",
    "* Independent Component Correlation Algorithm (ICA)\n",
    "* Gaussian Random Projection (GRP)\n",
    "* SparseRandomProjection (SRP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Make dimension reduction          #\n",
    "#####################################\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "#############################\n",
    "#  Parameter initialization #\n",
    "#############################\n",
    "n_comp = 12\n",
    "random_state = 420\n",
    "\n",
    "########\n",
    "# tSVD #\n",
    "########\n",
    "tsvd = TruncatedSVD(n_components=n_comp, random_state=random_state)\n",
    "tsvd_results_train = tsvd.fit_transform(train.drop([\"y\"], axis=1))\n",
    "tsvd_results_test = tsvd.transform(test)\n",
    "\n",
    "#######\n",
    "# PCA #\n",
    "#######\n",
    "pca = PCA(n_components=n_comp, random_state=random_state)\n",
    "pca2_results_train = pca.fit_transform(train.drop([\"y\"], axis=1))\n",
    "pca2_results_test = pca.transform(test)\n",
    "\n",
    "#######\n",
    "# ICA #\n",
    "#######\n",
    "ica = FastICA(n_components=n_comp, random_state=random_state)\n",
    "ica2_results_train = ica.fit_transform(train.drop([\"y\"], axis=1))\n",
    "ica2_results_test = ica.transform(test)\n",
    "\n",
    "#######\n",
    "# GRP #\n",
    "#######\n",
    "grp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=random_state)\n",
    "grp_results_train = grp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "grp_results_test = grp.transform(test)\n",
    "\n",
    "#######\n",
    "# SRP #\n",
    "#######\n",
    "srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=random_state)\n",
    "srp_results_train = srp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "srp_results_test = srp.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the decomposition components to original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save columns list before adding the decomposition components\n",
    "\n",
    "original_feats_col_name_list = list(set(train.columns) - set(['y']))\n",
    "\n",
    "#################################################\n",
    "# Append decomposition components to datasets   #\n",
    "#################################################\n",
    "for i in xrange(1, n_comp + 1):\n",
    "    train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n",
    "    test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n",
    "\n",
    "    train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n",
    "    test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n",
    "\n",
    "    train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n",
    "    test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n",
    "\n",
    "    train['grp_' + str(i)] = grp_results_train[:, i - 1]\n",
    "    test['grp_' + str(i)] = grp_results_test[:, i - 1]\n",
    "\n",
    "    train['srp_' + str(i)] = srp_results_train[:, i - 1]\n",
    "    test['srp_' + str(i)] = srp_results_test[:, i - 1]\n",
    "\n",
    "y_train = train['y'].values\n",
    "y_train_mean = np.mean(y_train)\n",
    "id_test = test['ID'].values\n",
    "\n",
    "#############################################\n",
    "# finaltrainset and finaltestset are data   #\n",
    "#     to be used only the stacked model     #\n",
    "#     (does not contain PCA, SVD... arrays) #\n",
    "#############################################\n",
    "finaltrainset = train[original_feats_col_name_list].values\n",
    "finaltestset = test[original_feats_col_name_list].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "# Train the xgb model #\n",
    "#######################\n",
    "xgb_params = {\n",
    "    'n_trees': 520, \n",
    "    'eta': 0.0045,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.93,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'base_score': y_train_mean, # base prediction = mean(target)\n",
    "    'silent': 0,\n",
    "    'early_stopping_rounds': 200,\n",
    "    'num_boost_rounds': 1250\n",
    "}\n",
    "num_boost_rounds = 1250\n",
    "\n",
    "# NOTE: Make sure that the class is labeled 'class' in the data file\n",
    "\n",
    "dtrain = xgb.DMatrix(train.drop('y', axis=1), y_train)\n",
    "dtest = xgb.DMatrix(test)\n",
    "\n",
    "# train model\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain,\n",
    "                       num_boost_round=num_boost_rounds)\n",
    "\n",
    "############################\n",
    "# Predict the test data    #\n",
    "#   based on trained model #\n",
    "############################\n",
    "y_pred = model.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train stacked models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# Train the stacked models #\n",
    "############################\n",
    "\n",
    "stacked_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=LassoLarsCV(normalize=True)),\n",
    "    StackingEstimator(estimator=GradientBoostingRegressor(learning_rate=0.001,\n",
    "                                                          loss=\"huber\",\n",
    "                                                          max_depth=3,\n",
    "                                                          max_features=0.55,\n",
    "                                                          min_samples_leaf=18,\n",
    "                                                          min_samples_split=14,\n",
    "                                                          subsample=0.7),\n",
    "                     ),\n",
    "    LassoLarsCV()\n",
    ")\n",
    "\n",
    "stacked_pipeline.fit(finaltrainset, y_train)\n",
    "\n",
    "#########################\n",
    "# Predict the test data #\n",
    "#########################\n",
    "\n",
    "results = stacked_pipeline.predict(finaltestset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result evaluation and result store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score on train data:\n",
      "0.659581560761\n"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "# R2 Score                      #\n",
    "#     on the entire             #\n",
    "#     train data when averaging #\n",
    "#################################\n",
    "\n",
    "print('R2 score on train data:')\n",
    "print(r2_score(y_train,stacked_pipeline.predict(finaltrainset)*0.2855 + model.predict(dtrain)*0.7145))\n",
    "\n",
    "###################################\n",
    "# Average the predition test data # \n",
    "#      of both models then save   #\n",
    "#      it on a csv file           #\n",
    "###################################\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = y_pred*0.75 + results*0.25\n",
    "sub.to_csv('stacked-models.csv', index=False)\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
